{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=teal size=6> <b> Multiple Linear Regression (cost function &  gradient descent) </b> </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data.\n",
    "data = pd.read_csv('housing_data.csv')\n",
    "\n",
    "# IN OUR PROGRAM WE ARE WORKING WITH ARRAY\n",
    "# Fetching input and ouput data\n",
    "x = data.iloc[:,:13].values\n",
    "y = data.iloc[:,13].values\n",
    "n,m = x.shape              # here, n gives the no. of rows and m gives the no. of columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In hypothesis function y = b0 + b1(x1) + b2(x2)... here we consider x0 = 1\n",
    "# Therefore we have to add an column of x0 considering its value as 1\n",
    "x_0 =  np.ones((n,1))             \n",
    "x = np.hstack((x_0,x))           # now concatenating both the arrays \n",
    "# In hstack, h is representing the horizontal stacking \n",
    "\n",
    "\n",
    "# Spliting the data\n",
    "# considering test_size = 0.4, describing that testing data will contain 40% of data\n",
    "# remaining 60% will move with training data\n",
    "train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.4,random_state=0)\n",
    "r,c = train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cost_function and gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent:  2.1491425573424346e+21\n",
      "Cost Function:  125126545.39556691\n"
     ]
    }
   ],
   "source": [
    "theta = np.ones((c,1))   # In starting, we can take random values of theta...here considering it as 1\n",
    "\n",
    "#(304,14)(14,1)\n",
    "def cost_func(train_x,train_y,theta):\n",
    "              length = (len(train_x))\n",
    "              summation = np.sum((np.dot(train_x,theta)-train_y)**2)\n",
    "              cost = summation/(2*length)\n",
    "              return cost\n",
    "        \n",
    "'''\n",
    "theta= vertical matrix of r rows and 1 column\n",
    "train_x=[x0,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13]\n",
    "         .\n",
    "         .\n",
    "         .\n",
    "         .\n",
    "         upto no. of rows\n",
    "hypothesis = []\n",
    "'''\n",
    "def grad_desc(train_x,train_y,theta):\n",
    "              alpha = 0.15\n",
    "              length = len(train_x)\n",
    "              hypothesis = np.dot(train_x,theta)\n",
    "              for _ in range(100):\n",
    "                            temp = theta - (alpha*(1/length)*(np.sum(hypothesis-train_y)))\n",
    "        \n",
    "                            theta = temp                            \n",
    "                            z = cost_func(train_x,train_y,theta)\n",
    "                            \n",
    "              return z\n",
    "\n",
    "print(\"Gradient Descent: \",grad_desc(train_x,train_y,theta))\n",
    "print(\"Cost Function: \",cost_func(train_x,train_y,theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
